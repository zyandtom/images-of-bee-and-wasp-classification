{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: loss = 0.87, train_acc = 0.7000, test_acc = 0.5333\n",
      "Epoch   10: loss = 0.39, train_acc = 0.7000, test_acc = 0.5333\n",
      "Epoch   20: loss = 0.96, train_acc = 0.7083, test_acc = 0.5667\n",
      "Epoch   30: loss = 0.55, train_acc = 0.8333, test_acc = 0.9333\n",
      "Epoch   40: loss = 0.27, train_acc = 0.8833, test_acc = 0.7667\n",
      "Epoch   50: loss = 0.30, train_acc = 0.9167, test_acc = 0.9333\n",
      "Epoch   60: loss = 0.27, train_acc = 0.9750, test_acc = 0.9333\n",
      "Epoch   70: loss = 0.23, train_acc = 0.9083, test_acc = 0.9333\n",
      "Epoch   80: loss = 0.23, train_acc = 0.9667, test_acc = 1.0000\n",
      "Epoch   90: loss = 0.26, train_acc = 0.9667, test_acc = 1.0000\n",
      "Epoch  100: loss = 0.43, train_acc = 0.8833, test_acc = 0.7667\n",
      "Epoch  110: loss = 0.23, train_acc = 0.9667, test_acc = 1.0000\n",
      "Epoch  120: loss = 0.41, train_acc = 0.9083, test_acc = 0.8667\n",
      "Epoch  130: loss = 0.26, train_acc = 0.9500, test_acc = 0.9333\n",
      "Epoch  140: loss = 0.31, train_acc = 0.9750, test_acc = 0.9333\n",
      "Epoch  150: loss = 0.32, train_acc = 0.9583, test_acc = 0.9667\n",
      "Epoch  160: loss = 0.23, train_acc = 0.9750, test_acc = 0.9333\n",
      "Epoch  170: loss = 0.38, train_acc = 0.9750, test_acc = 0.9333\n",
      "Epoch  180: loss = 0.30, train_acc = 0.9750, test_acc = 0.9333\n",
      "Epoch  190: loss = 0.35, train_acc = 0.9667, test_acc = 0.9667\n",
      "New Samples, Class Predictions:    [1 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Student Name: ZENG Yang\n",
    "\n",
    "Student ID: 20711899\n",
    "\n",
    "Assignment #: 1\n",
    "\n",
    "Student Email: yzengav@connect.ust.hk\n",
    "\n",
    "Course Name: Machine Learning\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "           Programming Assignment 1\n",
    "         \n",
    "\n",
    "This program learns a softmax model for the Iris dataset (included).\n",
    "There is a function, compute_softmax_loss, that computes the\n",
    "softmax loss and the gradient. It is left empty. Your task is to write\n",
    "the function.\n",
    "     \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "\n",
    "def get_data():\n",
    "    # Load datasets.\n",
    "    train_data = np.genfromtxt(IRIS_TRAINING, skip_header=1, \n",
    "        dtype=float, delimiter=',') \n",
    "    test_data = np.genfromtxt(IRIS_TEST, skip_header=1, \n",
    "        dtype=float, delimiter=',') \n",
    "    train_x = train_data[:, :4]\n",
    "    train_y = train_data[:, 4].astype(np.int64)\n",
    "    test_x = test_data[:, :4]\n",
    "    test_y = test_data[:, 4].astype(np.int64)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "def compute_softmax_loss(W, X, y, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function.\n",
    "    Inputs:\n",
    "    - W: D x K array of weight, where K is the number of classes.\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "    - y: 1-d array of shape (N, ) for the training labels.\n",
    "    - reg: weight regularization coefficient.\n",
    "\n",
    "    Returns:\n",
    "    - softmax loss: NLL/N +  0.5 *reg* L2 regularization,\n",
    "            \n",
    "    - dW: the gradient for W.\n",
    "    \"\"\"\n",
    " \n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the softmax loss and its gradient.                          #\n",
    "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "    # regularization!                                                           #\n",
    "    #############################################################################\n",
    "    #loss function\n",
    "    Z = np.exp(np.matmul(W.T, X.T)).sum(axis = 0)\n",
    "    loss = 0\n",
    "    for i in range(len(X)):\n",
    "        loss += np.matmul(W.T[y[i]], X.T[:, i]) - np.log(Z[i]) \n",
    "    loss = -1/len(X) *loss\n",
    "    \n",
    "    #L2 Regularization\n",
    "    loss = loss + 0.5 *reg *np.square(W).sum().sum()\n",
    "    \n",
    "    #Gradient\n",
    "    dW = np.zeros(np.shape(W))\n",
    "    for i in range(len(W[0])):\n",
    "        for j in range(len(X)):\n",
    "            if y[j] == i:\n",
    "                dW[:, i] += (1 - np.exp(np.matmul(W.T[i], X.T[:, j])) / Z[j]) *X.T[:, j]\n",
    "            else:\n",
    "                dW[:, i] += (0 - np.exp(np.matmul(W.T[i], X.T[:, j])) / Z[j]) *X.T[:, j]\n",
    "    dW = -1/len(X) *dW + reg *W\n",
    "    \n",
    "\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "\n",
    "    return loss, dW\n",
    "\n",
    "def predict(W, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict labels for\n",
    "    data points.\n",
    "\n",
    "    Inputs:\n",
    "    - W: D x K array of weights. K is the number of classes.\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO:                                                                   #\n",
    "    # Implement this method. Store the predicted labels in y_pred.            #\n",
    "    ###########################################################################\n",
    "    score = X.dot(W)\n",
    "    y_pred = np.argmax(score, axis=1)\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "    return y_pred\n",
    "\n",
    "def acc(ylabel, y_pred):\n",
    "    return np.mean(ylabel == y_pred)\n",
    "\n",
    "\n",
    "def train(X, y, Xtest, ytest, learning_rate=1e-3, reg=1e-5, epochs=100, batch_size=20):\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    num_iters_per_epoch = int(math.floor(1.0*num_train/batch_size))\n",
    "    \n",
    "    # randomly initialize W\n",
    "    W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        perm_idx = np.random.permutation(num_train)\n",
    "        # perform mini-batch SGD update\n",
    "        for it in range(num_iters_per_epoch):\n",
    "            idx = perm_idx[it*batch_size:(it+1)*batch_size]\n",
    "            batch_x = X[idx]\n",
    "            batch_y = y[idx]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, grad = compute_softmax_loss(W, batch_x, batch_y, reg)\n",
    "\n",
    "            # update parameters\n",
    "            W += -learning_rate * grad\n",
    "            \n",
    "\n",
    "        # evaluate and print every 10 steps\n",
    "        if epoch % 10 == 0:\n",
    "            train_acc = acc(y, predict(W, X))\n",
    "            test_acc = acc(ytest, predict(W, Xtest))\n",
    "            print('Epoch %4d: loss = %.2f, train_acc = %.4f, test_acc = %.4f' \\\n",
    "                % (epoch, loss, train_acc, test_acc))\n",
    "    \n",
    "    return W\n",
    "\n",
    "max_epochs = 200\n",
    "batch_size = 20\n",
    "learning_rate = 0.1\n",
    "reg = 0.01\n",
    "\n",
    "# get training and testing data\n",
    "train_x, train_y, test_x, test_y = get_data()\n",
    "W = train(train_x, train_y, test_x, test_y, learning_rate, reg, max_epochs, batch_size)\n",
    "\n",
    "# Classify two new flower samples.\n",
    "def new_samples():\n",
    "    return np.array(\n",
    "      [[6.4, 3.2, 4.5, 1.5],\n",
    "       [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "new_x = new_samples()\n",
    "predictions = predict(W, new_x)\n",
    "\n",
    "print(\"New Samples, Class Predictions:    {}\\n\".format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 24],\n",
       "       [40, 51]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[2,3],[5,6]])\n",
    "a = np.array([[2,3],[5,6]])\n",
    "np.dot(W,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[22, 28],\n",
       "       [49, 64]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "b = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "c = np.exp(b)\n",
    "d = b.sum().sum()\n",
    "e = np.log(b)\n",
    "f = np.square(b)\n",
    "g = np.array([2, 2, 2])\n",
    "h = np.argmax(b, axis=0)\n",
    "i = b[:,0]\n",
    "w = np.zeros(np.shape(b))\n",
    "z = np.array([[1,2],[3,4],[5,6]])\n",
    "print(b)\n",
    "print(z)\n",
    "np.matmul(b,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
